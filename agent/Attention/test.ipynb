{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_trading_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77c0f2fdf5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcustom_trading_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNTradingAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_agent\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'custom_trading_env'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from custom_trading_env import TradingEnv\n",
    "from utils import device\n",
    "import DQNTradingAgent.dqn_agent as dqn_agent\n",
    "from custom_hyperparameters import hyperparams\n",
    "from arguments import argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from custom_trading_env import TradingEnv\n",
    "from utils import device\n",
    "import DQNTradingAgent.dqn_agent as dqn_agent\n",
    "from custom_hyperparameters import hyperparams\n",
    "from arguments import argparser\n",
    "\n",
    "args = argparser()\n",
    "# device_num, save_num, risk_aversion, n_episodes, fee\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(args.device_num))\n",
    "dqn_agent.set_device(device)\n",
    "\n",
    "save_location = 'saves/Original/{}'.format(args.save_num)\n",
    "\n",
    "if not os.path.exists(save_location):\n",
    "    os.makedirs(save_location)\n",
    "\n",
    "save_interval  = 200\n",
    "print_interval = 1\n",
    "\n",
    "n_episodes   = args.n_episodes\n",
    "sample_len   = 480\n",
    "obs_data_len = 192\n",
    "step_len     = 1\n",
    "fee          = args.fee\n",
    "sell_at_end  = False\n",
    "\n",
    "risk_aversion_multiplier = 0.5 + args.risk_aversion / 2\n",
    "\n",
    "n_action_intervals = 5\n",
    "\n",
    "init_budget = 1\n",
    "\n",
    "torch.save(hyperparams, os.path.join(save_location, \"hyperparams.pth\"))\n",
    "\n",
    "df = pd.read_hdf('dataset/binance_data_train.h5', 'STW')\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    global sample_len\n",
    "\n",
    "    env = TradingEnv(custom_args=args, env_id='custom_trading_env', obs_data_len=obs_data_len, step_len=step_len, sample_len=sample_len,\n",
    "                     df=df, fee=fee, initial_budget=1, n_action_intervals=n_action_intervals, deal_col_name='c',\n",
    "                     sell_at_end=sell_at_end,\n",
    "                     feature_names=['o', 'h','l','c','v',\n",
    "                                    'num_trades', 'taker_base_vol'])\n",
    "    agent = dqn_agent.Agent(action_size=2 * n_action_intervals + 1, obs_len=obs_data_len, num_features=env.reset().shape[-1], **hyperparams)\n",
    "\n",
    "    beta = 0.4\n",
    "    beta_inc = (1 - beta) / 1000\n",
    "    agent.beta = beta\n",
    "\n",
    "    scores_list = []\n",
    "    loss_list = []\n",
    "    n_epi = 0\n",
    "    # for n_epi in range(10000):  # 게임 1만판 진행\n",
    "    for i_episode in range(n_episodes):\n",
    "        n_epi +=1\n",
    "        \n",
    "        # if (i_episode + 1) % 500 == 0:\n",
    "        #     sample_len += 480\n",
    "        #     env.sample_len = sample_len\n",
    "\n",
    "        state = env.reset()\n",
    "        score = 0.\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # for t in range(num_steps):\n",
    "        while True:\n",
    "            action = int(agent.act(state, eps=0.))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            score += reward\n",
    "            if reward < 0:\n",
    "                reward *= risk_aversion_multiplier\n",
    "            if sell_at_end and done:\n",
    "                action = 2 * n_action_intervals\n",
    "            actions.append(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            agent.memory.reset_multisteps()\n",
    "\n",
    "        beta = min(1, beta + beta_inc)\n",
    "        agent.beta = beta\n",
    "\n",
    "        scores_list.append(score)\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print_str = \"# of episode: {:d}, avg score: {:.4f}\\n  Actions: {}\".format(n_epi, sum(scores_list[-print_interval:]) / print_interval, np.array(actions))\n",
    "            print(print_str)\n",
    "            # with open(os.path.join(save_location, \"output_log.txt\"), mode='a') as f:\n",
    "            #     f.write(print_str + '\\n')\n",
    "\n",
    "        if n_epi % save_interval == 0:\n",
    "            torch.save(agent.qnetwork_local.state_dict(), os.path.join(save_location, 'TradingGym_Rainbow_{:d}.pth'.format(n_epi)))\n",
    "            torch.save(scores_list, os.path.join(save_location, 'scores.pth'))\n",
    "\n",
    "    del env\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
